Note: we'll show the developement of this "warts and all".

Want to make a minimum SQL database
- SQL-87 (nothing fancy)
- Simple datatypes
- 255 character varchar limit
- 32 column table limit
- One database per file
- No transactions
- No foreign keys or other constraints

1. DataModel
2. Execution
3. SQL Parser
4. Benchmarks against SQLite

Let's start from the bottom: columns.  They have a type and a name.  In the future they may have more

Then we define a row.  Rows have an array of column values

Finally, a table has an array of column types (up to a fixed limit) and a pointer to a bunch of rows.  Oh, and a table name.

OK, now that we have the basic definitions out of the way, we need an API to manipulate them.  First off, we need a way to create and drop tables.
We should try to have the API mimic the eventual schema definitions as closely as possible.

Note that we will not be writing especially secure or tight code.  In particular, you'll notice that the code will be especially susceptible to buffer overruns and leaks memory like a sieve.  If you are an experienced C programmer, you may cringe at some of this stuff, but the point is NOT to make a production-ready system, the point is to show clearly how one might build a database system from the ground up.

>>>>6941ff<<<<<

Autoincrement column type - but now we have the first operation that could generate an error due to user action (and not some program malfunction).  In this case, it's trying to set a value explicitly in the autoincrement column.  We also have another problem: to keep our API simple, we pass in a simple array of the values.  So how do we pass in a "value" that's not really a value?  It's time for an IGNORED type.  Next problem: how do we keep track of the last ID used for each?  It would be a pain to search the table everytime we needed it.  We could store it directly in the table, but what if they had more than one autoincrement column?  Solution: store it in the Column Def!  But it seems hacky to put something in there called "lastAutoincrementNumber".  I hate data members that are only valid for a small substate of the object.  Why don't we do something a bit more elegant and just always store the last inserted value in the Column def?  That would still serve our purpose, be far more elegant, and may come in handy for other uses down the road.  Perfect!

>>>>ee41a0e<<<<

Now that we can work with some basic table operations, need to create the concept of a database.  For our purposes, a "database" is simply a thing with a name and a bunch of tables.  This is easy enough to create, but we also need to modify the existing API calls to know about databases.  For example, when you create a table, you should create it IN a database.  While we're at it, let's make the API more consistent by returning Results instead of the objects.  The objects returned will be an out parameter. Had to rename InsertInto to InsertRow() because we will have an InsertTable() function.  We also have our first private helper method, so we differentiate it by naming style to show that it shouldn't be called as a public interface.  As an architecture note, we now notice we're doing the whole "see if we have any more chunks left, and if not allocate some more" behavior in two different places.  Only the names have changed.  We won't do anything about this now, but we take note of it and consider it as a possible refactoring to remove the repeated code.

Next, we need calls to drop the database and print it.

>>>>c5fc8dc<<<<

Before we start to implement some of the more complicated features, it's time to do some housecleaning.  So far we've been able to do everything in one file.  We don't even have a header file.  I highly recommend starting this way so as not to "over engineer" your program ahead of time just for the sake of picking the right file names for your classes/modules.  But now the program is getting bigger and it's fixing to get a whole LOT bigger.  We still don't need to start breaking out separate modules or anything, but what we DO need to do is separate the public API from the implementation and internal functions.  

So what should we call our first file?  Well everything we've done so far has been database engine internal stuff.  We can anticipate other things down the line that will use this interface (such as the SQL language parser).  So let's just call it "engine".

Now that we're thinking in terms of APIs for other programs to use, we need to be more considerate of the global namespace.  Renamed LIMIT_* to ZDB_LIMIT_* and RESULT_* to ZDB_RESULT_* and so forth.  For a real world example of an API that does this very well, check out GTK and GLib.

I also used this opportunity to get rid of a bad habit I had.  I learned most of my more "advanced" C programming from Andre Lamothe's classic "Tricks of the Game Programming Gurus".  He would always define structs like:

    typedef struct SomeType_tag
    {
        // ... struct stuff
    } SomeType, *SomeTypePtr;

    I have no problem with the typedef of a specific pointer type (though I don't use this style myself).  However, the declaration of the _tag is now archaic and unnecessary.  I've known this for a while but refactoring periods like these are an opportunity to work to change old habits.  So now my struct defs look like:

    typedef struct
    {
        // ... struct stuff
    } SomeType;

    MUCH cleaner.

Renamed all functions, constants, and datatypes.  Now it's time to move the code to the engine file.  Now look at main.c.  This is what you want to see.  A file that expands and gets a little messy while you're coming up with ideas and creating the code, then it gets smaller and tidier when you refactor.

It's also important to remember your audience when programming.  Unless you are programming the top-level part of an application (that actually shows stuff to the user, for example), the code you're writing will be used by other programmers, and they are your users.  Just as you should be considerate to end users by making your application user friendly, you should also be considerate to your programmer users by making a friendly API.  Proper and consistent refactoring is a great way to do this.  One reason is that the code is no longer sitting in front of you in the same file.  So now you get to play user and dogfood your own API.  Remember: just because the only other user of your code may be you, it doesn't matter.  It'll probably be long after you wrote the code and you'll be in a different frame of mind because you'll be coding at a different level of your codbase.  By kind to yourself.

>>>>9573ef5<<<<<

Basic select support.  The way we'll flesh this out is to start with the simplest possible query that involves a table ("SELECT * FROM table") and make sure that works.  Then we'll build upon that.  All queries will be executed using a "forward-only firehose" cursor technique.  In other words, until we need to do something more advanced, we'll just iterate one row at a time and see if it matches the query parameters.  If it does, we'll return the row, if not, we'll go on to the next one, signalling when we're at then end.  Now, keep in mind that the results of a SQL query look an awful lot like a table, so why not reuse the existing table infrastructure to form our results?

But we need to think ahead a little bit.  As we add more features to the query functionality, the internal data structures are going to get more and more complicated, and our simplistic "it's just a table" approach might not work any more.  It would surely suck to have to change a bunch of downstream code that depended on that.  That's why the query API is a perfect candidate for encapsulation.  In short, the structures that are manipulated will be opaque to the API users, and they will use function calls to manipulate them.

Also along the "thinking ahead" lines... the query API will no doubt be one of the lengthier bits of the code.  So let's think ahead and separate it into its own module from the start.  REFACTOR POINT: We can already see a problem.  If I'm the end user of this library, I don't want to have to remember to include "engine.h", "query.h", and whatever else we may think of.  So we need a top-level header that will make it easy for our users.

Note that even though we KNOW we're going to add the support for multiple tables in the query later, we only wrote it to support one now.  We do this because we have no idea what the implementation is going to look like yet, so why paint ourselves into a corner for no reason.

The Recordset object is essentially an iterator.  Since our query functionality right now just lets us loop through rows in one table, all we need to do to keep track of our position is maintain a current index.  It is obvious that this won't be sufficient for more advanced query functionality, but that's why we keep our recordset object opaque to the library users: it lets us change the underlying implementation without worrying that we'll break other code.

Since we're adding the Get*Value() functions, we immediately see our first instance of an area where modifications to one part of the code must be matched by modifications to another.  In this instance, adding a new datatype to our table engine should be followed by adding appropriate functions in the query engine.  We'll make sure to make a note of that in the engine module.

The Get*Value() functions are just convenience methods.  To keep the code DRY, we have one internal function, _getValue() that actually performs the work.  

Note the bad code "smell" in GetIntValue().  This is a clue that our ColType attributes will need more structure.  In particular, certain types are castable to others.  COLTYPE_AUTOINCREMENT is castable to COLTYPE_INT.  We mark it as a hack to let us know we need to address it later.

Now that we have basic query support, we need to test it.  This is a good time to introduce some functional testing.  We won't go overboard here.  We'll just start by creating a method in the main driver called TestBasicQuery().  Again, don't overcomplicate things by writing functionality before you need it.  This also gives us an opportunity to break out some of our "dumb" test code into more reusable chunks.

Sure enough, my very basic test code caught PLENTY of stupid errors.

>>>>f2e7e0b<<<<<

Now that we have basic query support (in the form of "SELECT * FROM table"), let's start adding some features.  You can think of the initial set of changes we can make as "stuff to the left of FROM" and "stuff to the right of FROM".  When I start adding features to a new bit of code, I always start with the simplest first.  These often end up providing the structural support for the more advanced features later.

You might think that adding a feature like "SELECT COUNT(*)" would be pretty easy.  After all, we're just going to iterate through the rows and keep a counter.  However, the question is how we are going to return the result.  Currently, we simply provide a pointer to the row.  However, this is a result that isn't the same as the table we're selecting from.  We'll run into the same problem with virtually all other queries that don't start with "SELECT * FROM table".  This suggests we'll need to put in place the machinery for better results collection.  Let's leave that off the table for now.

It's clearly easier to add a feature to the right of the "FROM".  This would only involve testing for something on the current row, and incrementing past it if it's false.  We'll start by adding the simple "=" comparison.

Again going with the simplest thing that could possibly work for the feature we're adding, we are going to add a ZdbQueryCondition object and functions to implement it.  For now, the fields in the structure will be exactly the ones we need to implement the "=" comparison.  We're going to go SLIGHTLY one step ahead and give it a type field so it can at least easily handle "=" "<" ">" and so forth.  When we need fancier query support, this structure will of course change completely.

Now that we have the query condition object, of course we need to add it to ethe query object and put in the right function to add a query condition.

Once again, our column type equivalence problem comes back to bite us.  Let's go ahead and do a basic refactor so we can at least move the hackiness to one spot. We'll introduce the notion of a "canonical type".

There, that's much better.  Not only have we added the notion of compatible types, but it cleans up several areas of the code and reduces the amount of cases we need in most switch statements to only canonical types.

The key to implementing our basic query conditions is the ZdbGetNextResult() function.  It should first TEST that the conditions pass before returning.  If it doesn't match, we'll recursively call GetNextResult() until one does or we reach the end.  Or at least, that's what I want to do.  Since I can't assume your compiler does Tail Call Optimization, I can't be sure that calling GetNextResult() recursively won't result in a stack overflow on some braindead compilers.  Thus, we'll be boring and use a while loop.

Now that we have the basic infrastructure in place, it's time to write our test.  Right now it fails (of course), so it's time to put in the infrastructure to actually support it (_matchesValue(), _matchesQuery(), and so forth).

With the proper infrastructure in place, we see that the tests look good and the correct rows are being returned for the given query conditions.

>>>>8418a54<<<<

Before we add any more query conditions, we can see that the code is starting to develop a bad "smell" when it comes to types.  Type specific switching code is all over the place, and if you need to add a type, poor you because you're going to have to go all over the code to look for it.  It's time to create a new types module that consolidates this stuff and makes it easier to maintain.  Keep in mind, at any point in the code we're not looking for perfection, just "just enough design" to make the code functional and as elegant as possible.  When we can start to detect that we're stretching the limits of our design, it's time to evolve it by refactoring.

I looks like the best way forward is to take the object-oriented approach.  We define a type "base class" and override it for different types.  But for now, we don't actually need any kind of inheritence (it's actually possible in C).  We just need some polymorphism.  Function pointers to the rescue!

Not only is the resulting code cleaner, easier to read, and easier to maintain, but it has a REALLY cool side-benefit: user-defined types!

Of course it's going to be slower than the more direct method we were doing, but we won't worry about that.  Now is the time to DESIGN, not to optimize.  And while we don't go out of our way to make things slow, we don't spend a lot of time optimizing things that, in the long run, may not matter.  Only a profiler can tell you where the bottlenecks REALLY are.

This will also change our conception of what a "row" is.  In our new scheme, a "row" is an amount of space allocated to hold the amount necessary for all the types.  A row gets a big chunk of space, and the "values" array becomes just a set of null pointers.

The other thing we're going to do is get rid of the "AUTOINCREMENT" type.  It was fine for starters, but it's been causing us a lot of problems.  With our new type system, we can simple define an optional function called NextValue() that types can implement if that makes sense.  Then we make AUTOINCREMENT a column def attribute and it's now valid on any type that implements NextValue()!  This also lets us get rid of our "canonical type" idea, which was a bit clunky.

For the standard type functions, we don't want to retype a bunch of stuff just to change the cast, so we'll be using templates.  Of course, C doesn't have templates, but for simple cases you can replicate it with macros.  Most advanced code we've written so far.  But it's not that hard to follow.  I wrote the macros by first writing them for a specific type (int) and then turning it into a macro.  That's much easier than trying to write the macro blind.  I guarantee you that you won't get it right because the compiler isn't giving you any assistance.

Now we just define the public interface to the type system.  This mostly consist of passthrough functions that call the actual type's specific function to do the work.

OK we have the type system coded for the basic types (int and float).  Let's test that before we expand the system to support boolean and varchar.

The tests all pass.  We need to do one more refactoring step while we're in here in anticipation for the work that's to come.  Right now we have an inconsistency in our naming.  For example, to create a type you call "ZdbTypeCreate()" but to create a query you call "ZdbCreateQuery()".  We want all of the function names to follow the Zdb<Module><Fn>() naming convention.  This is for consistency as well as easier browsing through the functions.

>>>>1394622<<<<

Now that we have the type system in place, it's time to integrate it into the existing code.  This will be somewhat of a major undertaking as it involves removing a lot of switch-based code and replacing it with calls to the type system.  Also, we need to change our notion of what a "row" is.  Right now it's a collection of ColumnVal entries, but these will no longer exist.  Rows will now get their size from the type specifications.  For now, we won't be supporting dynamic row sizing as the memory management is too complicated.  To reflect this, the first thing we'll do is add an error to ZdbTypeSizeof() if an actual instance value is passed in.  With this in place, we know each row in a table will be the same size.  This allows us to simplify the memory management a bit.

Before we implement the full type system, we'll start by replacing the simpler operations with the new counterparts.  One thing we can do right away is remove the "ignored" field from the ColumnVal struct.  This works out REALLY nicely for us because the value union is now the first field in the struct, allowing us to pass them directly into the new type API.

Now that the old type system switch code has been replaced by the new type system code, we need to implement the varchar type to support our test data.

>>>>63eefa9<<<<

Implement rest of type system including NextValue for autoincrement and sizeof for row sizes.  Get rid of ColumnVal struct.  The idea is that now a row will be a contiguous array of bytes.  The engine will use the sizeof information from each type to extract the appropriate part of the row and cast it to the appropriate type value.  This will let us get rid of the ColumnVal union, which will save a LOT of space and reduce complexity.

We now need a different way to insert row values.  It seems like the best idea is to implement a VARARGS function that lets us update the values.  Even better, calling this the "UpdateValues" function will take us one step closer to supporting update functionality.  Also, since we're going to be reading the values from strings anyway, 
    we'll make life easier on ourselves by assuming all arguments to the UpdateValues
    function are strings and call each type's FromValue function.

We make a note to ourselves that we should probably memoize the results of a couple of our helper functions, since they will be called lots of times and their information doesn't tend to change unless the table is ALTERed.

We notice now that the query API doesn't match the Engine API.  The query API takes void* values from actual types, whereas the engine API takes strings and converts them internally.  We make a note to adjust the query API to do the same.  In the meantime, we're going to have to do a HUGE hack in main by creating global variables for the query conditions, because otherwise we're passing in pointers to variables on the stack and storing them.  BAD.

But now we've fully implemented our new type system.  This will give us MUCH greater flexibility in the future.

>>>>702a8a6<<<<

First thing we need to do is add a test to make sure we can use ZdbUpdateRowValues to 
update an already existing row.  We quickly run into a snag: if the column is autoincrement, 
updating the row increments the column AGAIN.  We need some way to test if there have been values inserted.  Easiest way seems to be to keep the data pointer set to NULL until the first time we call Update.

To make testing easier we add an UpdateRowTestHelper function.  Doing this shows us how cumbersome it is to do the update manually, suggesting ideas for query and engine API simplification.  It also tells us we need a generic ZdbQueryGetValue() function since we don't know the types ahead of time.  Luckily, we already have one: _getValue()!

Also we need a non var-args version of UpdateRow that takes real values.  The current string-based function will likely move.  To facilitate this as well as an upcoming change, the types module REALLY needs a copy value function.  Which we will need to test

>>>>618e52b<<<<

Now that we have the CopyValue function in the types modules, it's time to fix a big wart int the query engine.  Right now, the AddQueryCondition function takes a void* as a value.  This has two big problems: it puts too much trouble on the API user to set up the arguments for the call, and (more importantly) it simply passes the void* straight to the query object.  This means that if the caller doesn't manage the memory properly, the value may have changed or been free'd before or during query execution.  We can fix both of these problems by having the AddQueryCondition take a string and managing our own memory in the query condition object.  This also brings the API closer to what the SQL parser will need, since it will just use strings.

The first thing we need to do is make a function that will clean up the memory our query object will use.  While we're at it, since users always create a query using ZdbQueryCreate (which calls malloc()), ZdbQueryFree will also free the pointer for the query itself.  With this in place, we can allocate space for query values without concern that they will leak (as long as the user calls ZdbQueryFree(), of course).

Created ZdbTypeNewValue() to make it easier to create new values from strings.

With the help of our ZdbTypeNewValue() helper function, this turned out to be an easy change and all our tests pass without issue.

Before we go any further, I think we've hit another refactor point.  As we implement more and more functionality it becomes increasingly important to ensure we don't break anything that already exists.  We have test functions in place, but they require us to manually read the output and make sure it looks right.  This is now unacceptably inprecise.  We want a simple "it passed" or "it failed" result... unambiguous.  For fun, we'll uses ncurses to color the output.

But a good test framework can be much better if it's easier to track down the errors that occur.  For that reason, the first thing we'll do is update our ZdbResult type to become a struct with more information.

Talk about the anonymous struct initialization technique for the result def lookup.

What do we require from our message framework:
    * We would like to be able to easily understand the error and the parameters that caused it without digging through the code and setting breakpoints
    * At least in debug mode, the errors should include the file, line, and any other source-related information we can use
    * We should be able to see when an error is the source of a later error
    * It should be easy to add and maintain the error messages
    * It should be easy to send off a message

We see that what we need is a message framework that can serve as both a logging utility and as an error message source.

We'll use the same technique as we did in the types module: declare a global struct with pointers to well-named members that will serve as an easily looked-up "enumeration" on the user side but is also our message meta-data on the implementation side. <-- Separate blog post about this technique

This may all seem like overkill, but trust me it's not.  Not only will this be absolutely essential for giving good information to the user about the status of their operation or any parsing errors for their SQL, but it will help to track down and debug internal errors MUCH more quickly.

We've added the message system, now it's time to expand ZdbResult to be more meaningful.  The first thing we need to do is extract the ZdbPrintMessage() function to add one that inserts into a string.  We will be storing the string with our results.

We want our ZdbResult to store the message that was generated.  We have a ZdbMessageDef but not an actual ZdbMessage.  That's OK, though.  The messages are just for diagnostic purposes so until there's a compelling reason to do otherwise, we'll just let the strings be the message objects.  What we WILL need to do, however, is augment ZdbResult to store the message as well as store the previous link (so we can have stacktrace-like behavior).

(NOTE: PULL CHECKPOINT COMMIT HERE TO TALK ABOUT THIS PREVIOUS APPROACH)

I'm write in the middle of this message functionality, and I can tell that I've made a mistake.  Returning these full-fledged message objects from every function will be a nightmare from a memory management perspective and just make writing future code harder to handle.  For this reason, I'm going to abandon this approach and go back to returning just decorated ints (ZdbResults) from functions.  The message module will now be separate and there will be a convenience function to print a message and return a result.  We will also add a log level to ensure that not EVERYTHING is printed.

This is a good example of stopping what you're doing before you go too deep down a rabbit hole.  You always need to make sure you're working on the right problem.  In this case, the messages framework was threatening to become a full-fledged exception framework and, while interesting, that's not what we're trying to write.

Also getting rid of ZdbResult.  Thinking back, the reason I typedef'd the int in the first place was because I anticipated passing more information inside the result.  The only reason I wanted to do this was to report the number of rows affected.  But I just realized that that's the ONLY extra piece of information I wanted to return.  Well in that case, there's a much simpler solution: just return ints and have the errors be negative values!  That way 0 means success but also "no rows affected" and everything above that is the number of rows affected (where that makes sense).  There, MUCH simpler!

I'll also use this opportunity to make better error messages.  For example, the "AUTOINCREMENT" error is too specific.  It's really a "VALUE ERROR", and I'm sure we'll have more of those later.  Also, tons of "INVALID OPERATION" messages are really "YOU SHOULDN'T PASS NULL" here, and we'll reflect that fact.

We're going to get rid of the message module at this point.  It's too complex for what we're trying to do.

The next thing we're going to do is revamp the test code in the main function. We really need to be able to quickly run tests and see at a glance if everything passed or if something failed... we don't need to be trying to interpret the text output to see if everything is OK.  To this end, we've added some simple macros... TEST_START, TEST_ASSERT, and TEST_PASS.  The critical one is TEST_ASSERT, it will evaluate a condition and exit the program with a fail message if the condition evaluates to false.  There's no need to continue testing if one of the tests fails, as our expectation is that they will all pass.  

We'll do a quick test to make sure the tests fail if something goes wrong by intentionally breaking something in the code.  As expected, that caused one of the tests to fail and the program exited.

Our test cases are missing an explicit test of the autoincrement functionality, so we need to add that.  We went ahead and added that to the "CreateTestDatabase()" function by performing a query on the new rows and making sure that the ID value was different for all the rows.  As before, we tested this by temporarily breaking the autoincrement functionality and verifying that the test failed.  You MUST do this if you write tests after the fact.

Expand the Results to include rows affected.  We did this by having ZdbEngineUpdateRowValues return "1" if it was successfull.  This indicates that 1 row is affected.  As we add more update, insert, and delete commands, we follow this convention.  Note that when we made this change, a bunch of tests failed, so we had to go back and update those tests to reflect the new return value.  This is a Good Thing!

NOTE: Project structure reorgnization occured after this point!  Got rid of XCode project and move to standard UNIX makefile format.

>>>>7863e33<<<<

Now that we've done all this work, let's go for a quick win.  We already have an EQ query condition, let's introduce LT, LTE, GTE, NE as well. First thing we had to do is refactor the test code to have an "AssertResultContains" helper function to make it easier for us to see whether we get back results we're looking for.  Our TestOneCondition() call now takes a value to check for and the number of rows that should be returned.  This allows us to say, for example, that there should be 3 rows returned > salary of 34000.  It has limitations, but it should work for now.

>>>>f47251f<<<<

Column selection <-- Needs a special type: ValueRef, which points to another data item.  We will NOT be including this in the Standard Types enum

COUNT(*) support
Make sure all the comments that denote error strings are prefixed with ERROR so we can search for them later
Better error messages in the result struct
Support for basic query parameters

Looks like we need a "Command" module that's similar to the "Query" module.  The "Engine" module works with raw types and values, while the "Query" and "Command" modules work with strings.

Support for more generic API in query results (e.g. GetColumnType(), GetNumColumns(), GetColumnName(), generic GetValue(), etc.).  Good opportunity for some C polymorphism since both Tables and Queries can use this same interface.

Print out the results in a pretty fashion.  Replace the current Print() methods.  This will be the describe interface.

Update support with queries
Alter table support <-- tricky with current implementation
Support for DBNULL in the data and querying (IS [NOT] NULL).  Let's make two special values: DBNULL and MISSING.  Once these are present, it will become invalid to ever pass a true NULL as a column value.  It'll be easy to introduce these to the type system as we can just use special pointers.

Delete support.  This will be a BIG change as it will entail moving to linked lists to represent rows.

>>>>COMMIT<<<<<

Joins in selects

>>>>COMMIT<<<<<

SQL parser

>>>>COMMIT<<<<<

REPL and read from file

>>>>COMMIT<<<<<

Benchmarks against SQLite

>>>>COMMIT<<<<<

Save/load database to file
Improvements based on benchmarks (indexing?)
Basic security review and wrapup
Annotated source (http://jashkenas.github.com/docco/ - jashkenas ROCKS) or rather, Rocco
Fun Stuff, maybe to do:
    - Auto indexing: automatically create and remove indexes based on common queries
    - Support for NoSQL-style storage by having linked tables.  Reuse MongoDB or orther NoSQL syntax.  Auto discovery and manipulation of table schema when documents created and updated
    - Simple multi-user server support
